{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c107a20-0264-4452-bc1f-1d4bbc4337a6",
   "metadata": {},
   "source": [
    "# [View this notebook in nbviewer](https://nbviewer.org/github/yardimcilab/wistan/blob/main/demo.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9a37d6-fd16-4f86-bf0a-7098fa9bea0e",
   "metadata": {},
   "source": [
    "# Anatomy of a Wistan pipeline: motivation\n",
    "For mass adoption, you should probably **make a tailored command-line interface for your pipeline.** This should expose **only** the\n",
    "parameters the user might actually want to tweak during normal use, supplying sensible defaults and descriptions of what they do.\n",
    "\n",
    "Here, we're going to look at the building blocks of the first pipeline built with Wistan. The pipeline will look more complicated, but \n",
    "this is just like taking the casing off your computer. If you want to build a computer, look at the hardware on the inside to see how it works.\n",
    "If you want to sell a computer, put it in a nice case. Analogously, wrap your pipeline in a nice\n",
    "\n",
    "It originates in the bioinformatics field of the study of chromatin conformation. The author was using a tool called [hicrep](https://github.com/dejunlin/hicrep)\n",
    "to compute sample-vs-sample reproducibility scores for a large number of data files. The challenge was that `hicrep` only allows one to to\n",
    "run a single sample-vs-sample comparison. Its output is a plaintext series of comments and per-chromosome reproducibility scores in a text file.\n",
    "\n",
    "The author wanted to batch `hicrep` comparisons on a large number of data files, compute the mean of the per-chromosome scores for each comparison,\n",
    "and produce a [clustermap](https://seaborn.pydata.org/generated/seaborn.clustermap.html) with row and column captions being the data file prefixes.\n",
    "Having a whole PhD in this subfield ahead of him, the author wanted a tool to make this conveniently in the future.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf43ef3-96e0-4f05-887f-27b32c4ada6e",
   "metadata": {},
   "source": [
    "# Initialize sanb\n",
    "Initialize sanb with the name of the notebook. This makes allows the pipeline to append its own outputs to this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e13e4c49-1fa4-4972-aaa8-6cea5468a03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sanb\n",
    "sanb.set_notebook(\"demo-before.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139dc21f-795f-4f75-904a-3d2761e67b61",
   "metadata": {},
   "source": [
    "# Download data\n",
    "Download data to be analyzed (390M)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadca992-055b-45cb-b683-f2af22d6a27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ~/data\n",
    "!wget -P ~/data/ https://ftp.ncbi.nlm.nih.gov/geo/samples/GSM4604nnn/GSM4604290/suppl/GSM4604290%5F990.iced.mcool\n",
    "!wget -P ~/data/ https://ftp.ncbi.nlm.nih.gov/geo/samples/GSM4604nnn/GSM4604276/suppl/GSM4604276%5F868.iced.mcool\n",
    "!wget -P ~/data/ https://ftp.ncbi.nlm.nih.gov/geo/samples/GSM4604nnn/GSM4604278/suppl/GSM4604278%5F1953.iced.mcool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0e30d6-ee6f-488d-a720-b99a68633702",
   "metadata": {},
   "source": [
    "# Install package and write analysis plugin script\n",
    "Install the [hicrep](https://github.com/dejunlin/hicrep) package, the cornerstone of our analysis.\n",
    "\n",
    "A single, small, specialized plugin transforms raw text from the hicrep analysis tool into the mean value we are interested in.\n",
    "\n",
    "It simply reads `stdin`, extracts any lines containing floating-point numbers, and prints their average to `stdout`.\n",
    "\n",
    "For easy sharing, we store the plugin in the Jupyter notebook itself and write it to a file for use in the main pipeline.\n",
    "\n",
    "Note that setting permissions is important!\n",
    "\n",
    "Yang, T., Zhang, F., Yardımcı, G. G., Song, F., Hardison, R. C., Noble, W. S., ... & Li, Q. (2017). HiCRep: assessing the reproducibility of Hi-C data using a stratum-adjusted correlation coefficient. Genome research, 27(11), 1939-1949."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9dea703b-908f-46cc-9419-38cb43576df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install hicrep > /dev/null\n",
    "hicrep_mean=r'''#!/usr/bin/env python3\n",
    "import sys, statistics, click\n",
    "\n",
    "hicrep_output = sys.stdin.read().split(\\\"\\\\n\\\")\n",
    "\n",
    "scc_scores = []\n",
    "for line in hicrep_output:\n",
    "    try:\n",
    "        scc_scores.append(float(line))\n",
    "    except ValueError:\n",
    "        continue\n",
    "click.echo(statistics.mean(scc_scores))'''\n",
    "!mkdir -p ~/scripts && echo \"$hicrep_mean\" > ~/scripts/hicrep_mean\n",
    "!chmod +x ~/scripts/hicrep_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10416475-aa5d-4112-bb1d-98c2700beda6",
   "metadata": {},
   "source": [
    "# Main pipeline\n",
    "We will examine this line by line. Note that for distribution, this is far too complicated an interface. We can easily develop a sleek command-line based interface for it, however. This would expose just the parameters of the pipeline that the user may wish to modify, such as the arguments to `hicrep` or the location of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fcda84-b63d-4723-82f1-722e93578153",
   "metadata": {},
   "outputs": [],
   "source": [
    "sanb.last = \"samb_cell0\"; sanb.lidx = sanb.lastidx()\n",
    "!mkdir -p scc\n",
    "!ls ~/data/*.mcool | itertools-cli combinations-with-replacement 2 | curry-batch \"echo {{1}}\" \"echo {{2}}\" \"pathlib-cli prefix {{1}}\" \"pathlib-cli prefix {{2}}\" > hicrep_inputs.txt\n",
    "!cat hicrep_inputs.txt | curry-batch \"hicrep {{1}} {{2}} scc/{{3}}_{{4}}.txt --h 1 --binSize 1000000 --dBPMax 5000000\" > /dev/null && cat hicrep_inputs.txt | curry-batch \"echo {{3}}\" \"echo {{4}}\" \"cat scc/{{3}}_{{4}}.txt\" > hicrep_results.txt\n",
    "!cat hicrep_results.txt | curry-batch \"echo {{1}}\" \"echo {{2}}\" \"echo '{{3}}' | ~/scripts/hicrep_mean\" | curry-batch \"echo {{1}}\" \"echo {{2}}\" \"echo {{3}}\" \"echo {{2}}\" \"echo {{1}}\" \"echo {{3}}\" | pandas-cli dataframe \"df = pd.DataFrame(df.values.reshape(2*df.shape[0], 3)).drop_duplicates().pivot(index=0, columns=1, values=2).apply(pd.to_numeric)\" > hicrep_data.yaml\n",
    "!datavis-cli load-dataframe hicrep_data.yaml df | nbformat-cli cell add {sanb.notebook} {sanb.lidx} --distance 1\n",
    "!datavis-cli clustermap df | nbformat-cli cell add {sanb.notebook} {sanb.lidx} --distance 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e237f17b-1ef5-4cc0-8e6e-9aeaa61b3a5d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Line 1\n",
    "```\n",
    "sanb.last = \"samb_cell0\"; sanb.lidx = sanb.lastidx()\n",
    "```\n",
    "This gives the cell a unique ID, stored in the variable `sanb.last`. The `sanb.lastidx()` function looks up in this notebook the index\n",
    "of a cell containing the phrase `sanb.last = [value of sanb.last]`. This retrieves the index of the current cell.\n",
    "\n",
    "## Line 2\n",
    "```\n",
    "!mkdir -p scc\n",
    "```\n",
    "We create a folder to store the 289 intermediateper-comparison output files our analysis tool will generate for a 17-sample cross-comparison.\n",
    "\n",
    "## Line 3\n",
    "```\n",
    "!ls ~/data/*.mcool | itertools-cli combinations-with-replacement 2 | curry-batch \"echo {{1}}\" \"echo {{2}}\" \"pathlib-cli prefix {{1}}\" \"pathlib-cli prefix {{2}}\" > hicrep_inputs.txt\n",
    "```\n",
    "The data to be compared has a file extension `.mcool`. We load all the filenames to be compared, then generate combinations with replacement.\n",
    "We then extend this to a list of four strings: the two filenames, plus the filename prefixes that we wish to use.\n",
    "We store this combination in `hicrep_inputs.txt` using a redirect for convenience and as a record of our output so far.\n",
    "\n",
    "## Line 4\n",
    "```\n",
    "!cat hicrep_inputs.txt | curry-batch \"hicrep {{1}} {{2}} scc/{{3}}_{{4}}.txt --h 1 --binSize 1000000 --dBPMax 5000000\" > /dev/null && cat hicrep_inputs.txt | curry-batch \"echo {{3}}\" \"echo {{4}}\" \"cat scc/{{3}}_{{4}}.txt\" > hicrep_results.txt\n",
    "```\n",
    "We load the list of filenames and prefixes. Note that this is not strictly necessary - we could have just piped the output from the previous line into this one.\n",
    "Next, we call the main `hicrep` command to compute reproducibility scores on all pairs of input files. For each list of `file1 file2 prefix1 prefix2` strings in the input:\n",
    " - `{{1}}` and `{{2}}` are replaced by `file1` and `file2`, and represent the input files.\n",
    " - `{{3}}` and `{{4}}` are replaced by `prefix1` and `prefix2`. `scc/{{3}}_{{4}}.txt` will be the location of the output file.\n",
    "We silence the message outputs from hicrep by redirecting them to `/dev/null`. The `hicrep` program saves its outputs to individual text files, so we need to retrieve them.\n",
    "We do this by once again piping our comparison filenames and prefixes and using `curry-batch` to load the raw output file contents using `cat`.\n",
    "Again, we save this output to `hicrep_results.txt` as a record of our work so far and for convenience, but this isn't strictly necessary.\n",
    "\n",
    "## Line 5\n",
    "```\n",
    "!cat hicrep_results.txt | curry-batch \"echo {{1}}\" \"echo {{2}}\" \"echo '{{3}}' | ~/scripts/hicrep_mean\" | curry-batch \"echo {{1}}\" \"echo {{2}}\" \"echo {{3}}\" \"echo {{2}}\" \"echo {{1}}\" \"echo {{3}}\" | pandas-cli dataframe \"df = pd.DataFrame(df.values.reshape(2*df.shape[0], 3)).drop_duplicates().pivot(index=0, columns=1, values=2).apply(pd.to_numeric)\" > hicrep_data.yaml\n",
    "```\n",
    "We use `curry-batch` to feed in our previous results into a custom Python script `hicrep_mean` tailor-made for this step, while preserving the filename prefixes for later captioning.\n",
    "We then use `curry-batch` to duplicate the result with the order reversed, so that we can conveniently produce a symmetric matrix. This gives us a six-element `col_m row_n value col_n row_m value` list.\n",
    "We then load this into a pandas dataframe and reshape it to get the desired square, symmetric matrix labeled with our captions. First, we append it to a `2nx3` matrix, with row caption, column caption,\n",
    "and value columns. As the main diagonal is duplicated, we drop duplicates. Then we create a pivot dataframe to get it into the desired square shape and convert the values to numeric. Finally, we save this\n",
    "to a data file `hicrep_data.yaml` for convenience.\n",
    "\n",
    "## Line 6\n",
    "```\n",
    "!datavis-cli load-dataframe hicrep_data.yaml df | nbformat-cli cell add {sanb.notebook} {sanb.lidx} --distance 1\n",
    "```\n",
    "Our computations are done, and all that remains is to create a figure in the interactive environment of our Jupyter notebook for easy tweaking.\n",
    "This function starts by leveraging the `sanb` package to add a new cell to the notebook immediately after the pipeline-running cell containing code\n",
    "to load our output data into a pandas DataFrame called `df`.\n",
    "\n",
    "## Line 7\n",
    "```\n",
    "!datavis-cli clustermap df | nbformat-cli cell add {sanb.notebook} {sanb.lidx} --distance 2\n",
    "```\n",
    "Finally, we add another cell immediately after the data-loading function that contains the function call\n",
    "to produce a Seaborn clustermap from our data. The arguments to that function are comprehensively and explicitly\n",
    "initialized to default values, and a link to the Seaborn clustermap API is printed for reference. The clustermap\n",
    "uses a perceptually-accurate, colorblind-friendly colormap from [colorcet](https://colorcet.com/download/index.html) by default. These parameters\n",
    "can be tweaked in the cell and the clustermap immediately reproduced.\n",
    "\n",
    "## Reload from Disk\n",
    "After these cells are produced by the pipeline cell, the notebook will need to be reloaded using `File/Reload Notebook From Disk`.\n",
    "\n",
    "![image](https://github.com/yardimcilab/wistan/assets/86805107/4a9afd91-d88b-4966-851d-d2d260068ea9)\n",
    "\n",
    "When run, the above cell adds two new cells (which will appear before this one).\n",
    " - A dynamically-generated data loader. This merely loads our output data from the main pipeline to a pandas DataFrame `df` and prints some summary statistics.\n",
    " - This is pipeline-generated boilerplate code to make a Seaborn clustermap displaying our results, which are now stored in `df`. We can modify the parameters to tweak the clustermap visualization.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc23a48-1bd3-484e-86af-e970f0e78f89",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "As you can see, outside the tiny analysis-specific `hicrep_mean` plugin script, all components of this pipeline are simple reusable parts.\n",
    "Although this pipeline is unwieldly in the exposed form presented here, it can be easily wrapped into a more convenient command line interface\n",
    "exposing just the variables the user needs to set. We provide it here as an example of how to build a useful Wistan pipeline."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
